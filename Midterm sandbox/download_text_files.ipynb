{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7350f40e-0f3a-41b0-8dcd-da721c4863c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from sec_edgar_downloader import Downloader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9888d49-c6e6-4a1c-b1df-ef81428eb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# places to put files - best practice chapter 2!\n",
    "\n",
    "os.makedirs(\"input\", exist_ok=True)\n",
    "os.makedirs(\"10k_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c466d4d9-4900-4c27-bbdb-e9a4064fca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample (somewhat simplistic option!)\n",
    "\n",
    "sample_csv = \"input/sp500_firms.csv\"\n",
    "if not os.path.exists(sample_csv):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    sample = pd.read_html(url)[0]\n",
    "    sample.to_csv(sample_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6935c-4492-424b-b805-4c064f943ced",
   "metadata": {},
   "source": [
    "## Issues to consider with our spider below (the loop that downloads all the 10-Ks)\n",
    "\n",
    "- [x] loop over cik or ticker?\n",
    "    - here, tickers, but BEWARE! \n",
    "- [x] too fast! (maybe?)\n",
    "    - slow the code down!\n",
    "- [x]  everytime I run dl.get(), it redownloads \n",
    "    - don't! if i already downloaded the APPL don't repeat\n",
    "- [x] progress unclear - use a progress tracker \n",
    "    - manual prints\n",
    "    - or tqdm()\n",
    "- [ ] huge memory requirments! on my computer it would be 15GB!\n",
    "    - option a: lets only keep html files, and delete the txt files as we go (reduces to ~2.5GB)\n",
    "    - option b: put all the html files in a zipfolder (**much** less space)\n",
    "    - option c: a+b\n",
    "    - option d: extract the text from the file AS you download, which reduces how much you save on your computer (instead of the whole raw file). You can even put those in a zip file if you want. The main issue is that at the beginning of a large scraping task, it's hard to know if your code to extract the text will work across all the files you want. Since \"hard drives are cheap\", I usually just grab all the data at the front, and deal with it after. \n",
    "- [x] huge memory requirments! on github \n",
    "    - gitignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2cc64-26e4-4abf-9d34-c916b09d8892",
   "metadata": {},
   "source": [
    "## This loop is a spider!\n",
    "\n",
    "The main thing left to figure out is how to delete the text files as you go. \n",
    "- Then do the loop for 10 firms.\n",
    "- Then 20. (Notice how the first 10 aren't downloaded again!)\n",
    "- Look at the file size growing. Multiply the size of the 10k_files folder by 500/20 to guesstimate the full size. I'm guessing around 2.5GB.\n",
    "- If you want to zip the files, one way to do that is to\n",
    "    - use `shutil` to zip the whole directory after you finish the downloads\n",
    "    - then delete the directory\n",
    "    - you'll have to modify the measure_risk code to open 10-Ks from the zip file using `ZipFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62b419c-4c5b-45c0-8af6-da91b4dabda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = Downloader('10k_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec6ccd0-8baa-49aa-ac9e-e1fc15b08b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "firms = [\"MSFT\", \"AMZN\", \"AAPL\", \"TSLA\"]\n",
    "\n",
    "for firm in tqdm(firms):\n",
    "\n",
    "    # an overly simply option that might be too restrictive on bolder projects\n",
    "    # just look to see if we have a folder for that firm's 10-K and assume\n",
    "    # that means we have the 10-k for the right date (we could write a way to\n",
    "    # verify that)\n",
    "    firm_folder = \"10k_files/sec-edgar-filings/\" + firm\n",
    "    if not os.path.exists(firm_folder):\n",
    "        dl.get(\"10-K\", firm, amount=1, after=\"2019-03-01\", before=\"2020-03-21\")\n",
    "\n",
    "    # pause - be nice to server (probably unneeded, the sec_edgar_downloader\n",
    "    # source code contains checks to limit speed)\n",
    "    sleep(0.5)\n",
    "\n",
    "    # if I have downloaded txt files inside firm_folder:\n",
    "    # get list of .txt in the firms folder\n",
    "    # print them! once you're printing them, you can just...\n",
    "    # delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f653d79-a930-442b-990e-44be710be9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
