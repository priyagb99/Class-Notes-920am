{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85189b99-386d-4bbe-8c0f-ded37309cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f95fae-2936-4331-be68-b13cddbab187",
   "metadata": {},
   "source": [
    "## Opening ONE file\n",
    "\n",
    "Given a path to the html file, we want to get a string of text we can search.\n",
    "\n",
    "**This sounds like a function! Input: filepath. Output: string.**\n",
    "\n",
    "_Note: This works a little differently than the example in 4.4.4.4 of the website, because the \"txt\" files are different than the html files sec_edgar_downloader downloads for us._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3f554-c226-4518-85a9-1a104a81dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracto(fname):\n",
    "    '''\n",
    "    LDSF's little worker function to help with getting the text from the \n",
    "    filing_details.html files produced by sec_edgar_downloader.\n",
    "    '''\n",
    "\n",
    "    # when you open a file in python, the file is added to the RAM working memory\n",
    "    # so you want to close the file when you are done with it (otherwise you will have\n",
    "    # all 500 files open at the same time!)\n",
    "\n",
    "    # \"with\" automatically closes the file once the block of code insideit ends\n",
    "    # which reduces RAM/memory requirements, \n",
    "    with open(fname, encoding=\"utf-8\") as report_file:\n",
    "        html = report_file.read()\n",
    "        # now it automatically closes the 10-K html\n",
    "\n",
    "    # BeautifulSoup is one good way deal with the structure of this file\n",
    "    # it takes the html, parses according it to some rules (xml), \n",
    "    # then creates an object called soup \n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "    # this for-loop isn't on the website, but the txt file doesn't have all the xbrl crap\n",
    "    # I looked at the HTML manually and noticed the first <div> is a large (hidden!) chunk of xbrl data, \n",
    "    # let's delete it\n",
    "    for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "        div.decompose()\n",
    "\n",
    "    # now, we get the text, using the good ideas from 4.4\n",
    "    lower = soup.text.lower()\n",
    "    no_punc = re.sub(r'\\W',' ',lower)\n",
    "    cleaned = re.sub(r'\\s+',' ',no_punc).strip()    \n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ead72f-8de2-47ef-b4e8-9653ab3d5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"10k_files/sec-edgar-filings/TSLA/10-K/0001564590-20-004475/filing-details.html\"\n",
    "\n",
    "text = extracto(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a819e-87a8-4596-8396-1d942f4f48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text)\n",
    "# len(re.findall(' ',text))+1   # wordcount based on how extracto works is # of spaces + 1\n",
    "# text[:1000]\n",
    "# text[-1000:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
